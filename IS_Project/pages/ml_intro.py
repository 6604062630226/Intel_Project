# -*- coding: utf-8 -*-
"""svm.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1_XUrSHTs0KCNmHOi69m9JUbtZxt3qqzr
"""


#Installs the ydata-profiling package.
from ydata_profiling import ProfileReport
#Imports  ProfileReport class ‡∏à‡∏≤‡∏Å ydata_profiling library

# Commented out IPython magic to ensure Python compatibility.
import streamlit as st
import numpy as np
import tensorflow as tf
from tensorflow import keras
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm, datasets
import sklearn.model_selection as model_selection
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split, permutation_test_score
import seaborn as sns
import pickle as p
from sklearn.ensemble import VotingClassifier, RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import StandardScaler,MinMaxScaler,RobustScaler
from sklearn.linear_model import LogisticRegression
# %matplotlib inline

st.markdown("# machinelearning üéà")
st.sidebar.markdown("# machinelearning üéà")

st.title("üîç Machine Learning Intro")
st.write(""" 
         ### Heart Disease
        I founded out I want to do some model with some kind of disease but I don't have any ideas about disease.
        But I just found Heart Disease datasets from [this](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction/data). It contains 12 columns or 11 features with one label.
        People with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia 
        or already established disease) need early detection and management wherein a machine learning model can provide a great help.
        """
)
with st.expander("The 11 features are:"):
    st.write("""
    
    * Age
        * Description: The age of the individual.
        * Age is one of the most important factors in predicting heart disease. Older individuals are at higher risk for heart disease.
    * Sex
        * Description: The sex of the individual. Usually encoded as a categorical variable (e.g., Male = M, Female = F).
        * Sex is a significant predictor for heart disease risk, as certain heart disease types may be more common in one sex than the other.
    * ChestPainType (e.g., ST_Slope)
        * Description: Type of chest pain experienced by the individual. The categories might include:
            * ASY: Asymptomatic
            * TA: Typical Angina (chest pain from reduced blood flow to the heart)
            * ATA: Atypical Angina
            * NAP: Non-Anginal Pain
        * Chest pain is a common symptom of heart disease and can help identify individuals at risk.
    * RestingBP
        * Description: The individual's resting blood pressure (measured in mmHg).
        * High blood pressure (hypertension) is a significant risk factor for heart disease.
    * Cholesterol
        * Description: The level of cholesterol in the individual's blood(measured in mm/dl).
        * High cholesterol levels increase the risk of plaque buildup in the arteries, which can lead to heart disease.
    * FastingBS
        * Description: The level of blood sugar after fasting (measured in mg/dL).
        * Elevated fasting blood sugar levels may indicate diabetes, which is a risk factor for heart disease.
    * RestingECG
        * The results of an electrocardiogram (ECG) at rest. The possible categories might be:
            * Normal: Normal ECG
            * ST: ST-T wave abnormality
            * LVH: Left ventricular hypertrophy
    * MaxHR
        * Description: maximum heart rate achieved
        * A lower maximum heart rate may suggest heart disease, as the heart may struggle to increase its rate during exercise.
    * ExerciseAngina
        * Description: Whether the individual experiences angina (chest pain) when exercising.[Y: Yes, N: No]
         This is a strong indicator of heart disease, as exercise-induced angina can indicate coronary artery disease.
    * Oldpeak
        * Numeric value measured in depression
    * ST_Slope
        * the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]
""")
        
st.write("### Data Preparation")
st.write(""" The dataset is read from a CSV file (heart.csv), and the first few rows are displayed using
         
         df.head() 
         

         """ )

from PIL import Image as img
st.image(img.open("images/ml/df_head.png"))
st.write("""Checking columns type:
        
        df.dtypes
       
        """)
st.image(img.open("images/ml/df_dtype1.png"))
st.write("""Checking Missing Values:
        
        df.isnull().sum()
          
         """)
st.image(img.open("images/ml/df_isnull1.png"))
st.write("""Columns with data type "object" (i.e., categorical data) are selected and converted to string type for consistency.
         
        string_col = df.select_dtypes(include="object").columns
        df[string_col]=df[string_col].astype("string")
        
          
        
         """)
st.write("and we check with df.dtypes and df.isnull().sum again.")

st.image(img.open("images/ml/df_dtype2.png"))
st.image(img.open("images/ml/df_isnull2.png"))

st.code((""" 
         string_col=df.select_dtypes("string").columns.to_list()
         num_col=df.columns.to_list()
         for col in string_col:
            num_col.remove(col)
            num_col.remove("HeartDisease") 
         """),language="python") 

st.write(""" The columns are separated into string_col (categorical) and num_col (numeric) for later processing. 
         The target variable HeartDisease is removed from the num_col list since it's the label.""")

st.write("""and summarizing it with 
            ```py df.describe().T ```
         """)
st.image(img.open("images/ml/df_describe1.png"))
st.write("""### One-Hot Encoding for Categorical Features
         
    columns_to_label_encode = ['ST_Slope', 'Sex']
    for column in df.select_dtypes(include=['string']).columns:
        dummies = pd.get_dummies(df[column], prefix=column, drop_first=False, dummy_na=False)
        df = pd.concat([df, dummies], axis=1)
        df.drop(columns=[column], inplace=True)
         
    le = LabelEncoder()
    for column in columns_to_label_encode:
        if column in df.columns:
            df[column] = le.fit_transform(df[column])

          
         """)
st.write("""One-hot encoding is applied to categorical columns (in this case, all string columns).
        The columns are transformed into multiple binary columns (dummy variables) and the original columns are dropped.
         Label encoding is applied specifically to columns listed in columns_to_label_encode (such as ST_Slope and Sex). 
         It transforms categorical labels into numeric labels (0, 1, 2, etc.).""")
       

st.write(""" ### üìàCorrelation Heatmap:
         px.imshow(df.corr(), title="Correlation Plot of the Heart Failure Prediction")      """)
st.image(img.open("images/ml/corr_plot1.png"))
st.write("""A correlation plot is generated to visualize the correlation matrix between the features in the dataset. 
         This helps understand which features are correlated with each other.""")
st.write(""" ### Data Preprocessing (Scaling)""")
st.write("I use RobustScaler to the features (x) to make the model more robust to outliers. The label HeartDisease is separated into y, and the remaining columns are used as features (x).")



st.code(("""scaler = RobustScaler()
            y = df["HeartDisease"]
            x = df.drop(columns=["HeartDisease"])
            x = pd.DataFrame(scaler.fit_transform(x), columns=x.columns)
         
         """), language="python")

st.image(img.open("images/ml/x_scaled.png"))
st.image(img.open("images/ml/y_scaled.png"))
st.write("""The dataset is split into training and testing sets. The model will be trained on the training set and evaluated on the testing set.
         
         
         
            x_train, x_test, y_train, y_test = train_test_split(x, y)
  
        
         """)

st.write("### Model Used ")
st.write(""" I'm ensembling Random Forest Classifier and Support Vector Classifier (SVC) 
         because combining multiple models often improves performance, especially when the models have different strengths and weaknesses
        The Random Forest Classifier is a robust ensemble learning algorithm that leverages multiple decision trees to enhance predictive accuracy and control overfitting. 
         It operates by constructing a multitude of decision trees during training and outputting the class that is the mode of the classes (classification) 
         or mean prediction (regression) of the individual trees.  \n"""
        
        """Source: [https://medium.com/%40mrmaster907/introduction-random-forest-classification-by-example-6983d95c7b91](https://medium.com/%40mrmaster907/introduction-random-forest-classification-by-example-6983d95c7b91)\n"""
        )     
st.write("""A Support Vector Classifier (SVC) is a supervised machine learning algorithm used for classification tasks. 
        It aims to find the optimal hyperplane that best separates data points of different classes in a feature space. 
        The goal is to maximize the margin‚Äîthe distance between the hyperplane and the nearest data points from each class, known as support vectors. 
        This maximization enhances the model's generalization capabilities on unseen data.
        And I can selecting an appropriate kernel and tuning parameters like C and gamma is crucial for optimal performance. I started with Random Forest.  
         
         """)


         
st.code((""" forest = RandomForestClassifier(n_estimators=200, criterion="entropy")
             forest.fit(x_train, y_train)
             y_pred = forest.predict(x_test)
             print(classification_report(y_test, y_pred)) """), language="python")   
         
         
st.write("and the classification report (precision, recall, F1-score) is printed.")               
         
         
         
         
         
         
         
        
st.image(img.open("images/ml/forest_class_rep.png"))
st.write(""" And followed with Support Vector.""")

st.code(("""svm = SVC(kernel="linear", probability=True)
            svm.fit(x_train, y_train)
            y_pred = svm.predict(x_test)
            print(classification_report(y_test, y_pred))"""), language="python")

st.write("""             and the classification report (precision, recall, F1-score) is printed.
         """)

st.image(img.open("images/ml/svm_class_rep.png"))
st.write("I using Voting Classifier as Ensemble learning")
st.code(("""
           ensemble_model = VotingClassifier(estimators=[('svm', svm), ('forest', forest)], voting='soft')
            ensemble_model.fit(x_train, y_train)
            probabilities = ensemble_model.predict_proba(x_test)[:, 1]
            threshold = 0.5
            predictions = np.where(probabilities >= threshold, 1, 0)
            print(classification_report(y_test, y_pred))"""), language="python")
st.write("""             voting='soft' means the final prediction is based on the probability of the predicted class.
                The threshold is set to 0.5 for predicting the class, and the classification report is printed.
         
         """)
st.image(img.open("images/ml/ensemble_class_rep.png"))
st.write("""The confusion matrix is printed, which shows the counts of true positives on top-left, true negatives on bottom-right, 
         false positives on bottom-left, and false negatives on top-right.""")
st.image(img.open("images/ml/confusion_matrix.png"))

st.page_link("pages/ml_demo.py", label="try demo")